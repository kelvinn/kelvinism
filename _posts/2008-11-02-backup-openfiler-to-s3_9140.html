---
layout: post
title: Backup OpenFiler to S3
date: '2008-11-02T21:30:00.010+11:00'
author: Kelvin Nicholson
tags:
- howtos
- openfiler
modified_time: '2013-01-09T20:49:06.793+11:00'
blogger_id: tag:blogger.com,1999:blog-3439832858234004835.post-8583725903888314201
blogger_orig_url: https://www.kelvinism.com/2008/11/backup-openfiler-to-s3_9140.html
---

Backing up your Openfiler box to S3<br /><br /><p>While I don't think most pople would expect to backup their entire NAS/SAN to Amazon's S3, there might be a few very crucial things you need to backup.  For instance, my girlfriend's PhD papers and data.</p><p>I've seen an implementation using Ruby and s3sync -- something that I do on my server -- but I'm trying to migrate everything to Python.  Although there are a lot of great tools out there for S3, many of them Python-based,  I wanted to do one thing and do it well: have one complete full backup available, and using as little bandwidth as possible.  In these regards Duplicity would work well, except I wanted the ability to browse the S3 store using any other tool.</p><p>I've digged deeper into <a href="http://s3tools.logix.cz/s3cmd">s3cmd</a>, which I had noticed a long time ago, but I failed to notice it has a sync option.  I have tested it out, and it appears to work very, very well.  Here's how to use it with OF.</p><p>First, download <a href="http://s3tools.logix.cz/download">s3cmd</a>.  You'll need to use subversion, so I first checked it out to my laptop, then uploaded it via SSH to OF. I put my s3cmd folder in /opt.</p><pre class="brush: plain; light: true; "><br />[root@files opt]# ls<br />openfiler  s3cmd<br />[root@files opt]# <br /></pre><br /><br /><p>If you don't have elementtree installed, now is a good time to install it.</p><pre class="brush: plain; light: true; "><br />conary update elementtree:python<br /></pre><br /><br /><p>We need to next configure s3cmd with our AWS creds.</p><pre class="brush: plain; light: true; "><br />[root@files s3cmd]# ./s3cmd --configure<br /></pre><br /><br /><p>In the end I didn't configure encryption for my files (so just hit enter), but you may choose to do so.  I have configured the transfer to use HTTPS, however.</p><pre class="brush: plain; light: true; "><br />Save settings? [y/N] y<br />Configuration saved to '/root/.s3cfg'<br /></pre><br /><br /><p>Cool. Now create a bucket on S3 for your NAS, e.g. blah2134accesskey.openfiler, using whatever method you choose (I typically use Cockpit). Now that you have a bucket, configure a *really* simple script to drop in cron:</p><pre class="brush: bash; light: true; "><br />#!/bin/bash<br /><br />/opt/s3cmd/s3cmd sync /mnt/openfiler/data/profiles/bunny s3://blah2134accesskey.openfiler/mnt/openfiler/data/profiles/bunny<br />/opt/s3cmd/s3cmd sync /mnt/openfiler/data/profiles/kelvin-pc s3://blah2134accesskey.openfiler/mnt/openfiler/data/profiles/knicholson/kelvin-pc<br /></pre><br /><br /><p>Sweet! I like this approach quite a bit: I get file-level access to anything on the NAS, you don't have to actually install anything, and it 'just works.'</p>